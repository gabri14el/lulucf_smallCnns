# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yvYfTteXukO4l6_kXn1h3o077ftpeba0
"""


import os

#EXPERIMENT CONFIGURATION
os.chdir(r'C:\Users\Gabriel\OneDrive - Universidade de Tras-os-Montes e Alto Douro\UTAD\Prague')
dim = (5, 5, 13, 1)
nclasses = 6
batch_size = 64
mlflow_activate = True
experiment_name = 'Contrastive3DExperiment'
path = r'C:\Users\Gabriel\OneDrive - Universidade de Tras-os-Montes e Alto Douro\UTAD\Prague\data_5_v5_Lines_aug'
comments = 'retraining normally'
#SAVE PARAMS
import datetime
import utils

time = datetime.datetime.now().strftime("%Y%m%d-%H%M")
model_save_name = experiment_name+'_model_'+'_'+time
weight_save_name = experiment_name+'_weight_'+'_'+time+'.h5'
path_model = F"models\\{experiment_name}\\{model_save_name}"
path_weight = F"models\\{experiment_name}\\{weight_save_name}"

os.makedirs(F"models\\{experiment_name}", exist_ok=True)

import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow as tf 
import os
import pandas as pd
import mlflow
import mlflow.keras
import matplotlib.pyplot as plt
from sklearn import metrics
import itertools
import random
import warnings

#IMPORT THE DATASET
#path = r'C:\Users\Gabriel\OneDrive - Universidade de Tras-os-Montes e Alto Douro\UTAD\Prague\data2'
list_rows = []
for root, dirs, files in os.walk(path):
    for file in files:
        if file.endswith(".txt"):
             dic = {}
             dic['path'] = os.path.join(root, file)
             dic['name'] = dic['path'].split(os.path.sep)[-1]
             dic['class'] = dic['path'].split(os.path.sep)[-2]
             dic['set'] = dic['path'].split(os.path.sep)[-3]
             list_rows.append(dic)
df = pd.DataFrame.from_records(list_rows)
print(df.head())

df['class'] = df['class'].astype(int)
df['class'] = df['class'].values - 1

#get the classes' names
classes = np.unique(df['class'].values)



def get_pairs(df, train_classes, images_per_class, percent_positives, set_filter=None):
  list_rows = []
  for classe in train_classes:
    other_classes = np.array(train_classes)
    other_classes = other_classes[other_classes != classe]
    
    imgs_classe = df[df['class'] == classe]['path'].values
    imgs_classe_bkp = np.copy(imgs_classe)
    #negative cases
    qtd_negative = int(images_per_class*(1-percent_positives)/(len(train_classes)-1))
    qtd_postive = int(images_per_class*(percent_positives))

    #if qtd_postive > len(imgs_classe)/2:
        #qtd_postive = int(len(imgs_classe)/2)
        #warnings.warn('The class \'{}\' have a small number of elements, working with {} positve samples'.format(classe, qtd_postive))
    
    for i, _class in enumerate(other_classes):
      imgs_others = df[df['class'] == _class]['path'].values
      #qtd_negative_for_this_class = qtd_negative if len(imgs_others) <= qtd_negative else len(imgs_others)
      #print(_class, len(imgs_others))
      imgs_others_bkp = np.copy(imgs_others)
      
      while len(imgs_others) < qtd_negative:
        imgs_others = np.concatenate((imgs_others,imgs_others_bkp), axis=0)
        random.shuffle(imgs_others)
        print('The class \'{}\' have a small number of elements, appending it for {}\'s negatives'.format(_class, classe))
      for j in range(qtd_negative):
        index1 = int(random.random()*(len(imgs_classe)-1))
        index2 = int(random.random()*(len(imgs_others)-1))

        row = {}
        row['x1'] = imgs_classe[index1]
        row['x2'] = imgs_others[index2]
        row['y'] = 0.
        
        imgs_classe = np.delete(imgs_classe, index1)
        imgs_others = np.delete(imgs_others, index2)

        if len(imgs_classe) < 1:
            imgs_classe = np.copy(imgs_classe_bkp)
            random.shuffle(imgs_classe)
            print('The class \'{}\' have a small number of elements, restarting it for its negatives'.format(classe))
        list_rows.append(row)
    
    imgs_classe = df[df['class'] == classe]['path'].values

    #rpt = len(imgs_classe)/2 < qtd_postive
    while len(imgs_classe)/2 < qtd_postive:
        random.shuffle(imgs_classe_bkp)
        print('The class \'{}\' have a small number of elements ({}) , duplicating it for its postives, now it has {} images'.format(classe, len(imgs_classe), len(imgs_classe)+len(imgs_classe_bkp)))
        imgs_classe = np.concatenate((imgs_classe, imgs_classe_bkp), axis=0)
        

    for i in range(qtd_postive):
      row = {}
      index1 = int(random.random()*(len(imgs_classe)-1))
      row['x1'] = imgs_classe[index1]
      imgs_classe = np.delete(imgs_classe, index1)

      index2 = int(random.random()*(len(imgs_classe)-1))
      row['x2'] = imgs_classe[index2]
      
      while row['x1'] == row['x2']:
        index2 = index2 + 1
        row['x2'] = imgs_classe[index2]
      imgs_classe = np.delete(imgs_classe, index2)
      
      row['y'] = 1.
      list_rows.append(row)
  return pd.DataFrame.from_records(list_rows)


images_per_class = 3500
percent_positives = .2
df_filtred = df[df['set'] == 'train']

train_df = get_pairs(df_filtred, classes.copy(), images_per_class, percent_positives)

images_per_class = 450
percent_positives = .2

df_filtred = df[df['set'] == 'validation']

val_df = pd.DataFrame(columns=['x1', 'x2', 'y'])
val_df = get_pairs(df_filtred, classes.copy(), images_per_class, percent_positives)

images_per_class = 150
percent_positives = .2
df_filtred = df[df['set'] == 'test']
test_df = get_pairs(df_filtred, classes.copy(), images_per_class, percent_positives)

train_ids = train_df.index.values
train_classes = {}
train_values = {}

print(train_df.head(-5))
for i, row in train_df.iterrows():
    train_classes[i] = row['y']
    train_values[i] = (row['x1'], row['x2'])

print(train_values[0])

val_ids = val_df.index.values
val_classes = {}
val_values = {}

for i, row in val_df.iterrows():
    val_classes[i] = row['y']
    val_values[i] = (row['x1'], row['x2'])

test_ids = test_df.index.values
test_classes = {}
test_values = {}

for i, row in test_df.iterrows():
    test_classes[i] = row['y']
    test_values[i] =(row['x1'], row['x2'])


#PRE-PROCESSING
#max value of uint16
max = 65535

def pre_process(X):
    return X/(max+0.0)

#LOSS CONTRASTIVE LOSS
def loss(margin=1):
    """Provides 'constrastive_loss' an enclosing scope with variable 'margin'.

  Arguments:
      margin: Integer, defines the baseline for distance for which pairs
              should be classified as dissimilar. - (default is 1).

  Returns:
      'constrastive_loss' function with data ('margin') attached.
  """

    # Contrastive loss = mean( (1-true_value) * square(prediction) +
    #                         true_value * square( max(margin-prediction, 0) ))
    def contrastive_loss(y_true, y_pred):
        """Calculates the constrastive loss.

      Arguments:
          y_true: List of labels, each label is of type float32.
          y_pred: List of predictions of same length as of y_true,
                  each label is of type float32.

      Returns:
          A tensor containing constrastive loss as floating point value.
      """
        square_pred = tf.math.square(y_pred)
        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))
        return tf.math.reduce_mean(
            (1 - y_true) * square_pred + (y_true) * margin_square
        )
        
    return contrastive_loss

#TRAINING
train_params = {'ids':train_ids, 'labels':train_classes, 'values':train_values, 'batch_size':batch_size, 'preprocessing':pre_process}
train_gen = utils.ContrastiveDataGenerator(**train_params)
print(len(train_ids))

val_params = {'ids':val_ids, 'labels':val_classes, 'values':val_values, 'batch_size':batch_size, 'preprocessing':pre_process}
val_gen = utils.ContrastiveDataGenerator(**val_params)

test_params = {'ids':test_ids, 'labels':test_classes, 'values':test_values, 'batch_size':1, 'preprocessing':pre_process}
test_gen = utils.ContrastiveDataGenerator(**test_params)   
    
import tensorflow_addons as tfa
# example of a 3-block vgg style architecture
_input = tf.keras.layers.Input(shape=(dim[0],dim[1],dim[2],dim[3],))
x = tf.keras.layers.Conv3D(filters=32, kernel_size=3, activation="relu", padding="same")(_input)
x = tf.keras.layers.Conv3D(filters=32, kernel_size=3, activation="relu", padding="same")(x)
x = tf.keras.layers.MaxPool3D(pool_size=2)(x)
x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Conv3D(filters=32, kernel_size=3, activation="relu", padding="same")(x)
x = tf.keras.layers.Conv3D(filters=32, kernel_size=3, activation="relu", padding="same")(x)
x = tf.keras.layers.MaxPool3D(pool_size=2)(x)
x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)

embbeding = tf.keras.models.Model(inputs=_input, outputs=x)

input1=tf.keras.layers.Input(shape=(dim[0],dim[1],dim[2],dim[3],))
input2=tf.keras.layers.Input(shape=(dim[0],dim[1],dim[2],dim[3],))

encod1 = embbeding(input1)
encod2= embbeding(input2)

distance = tf.keras.layers.Dot(axes=1, normalize=True)([encod1, encod2])

siamese_net = tf.keras.models.Model(inputs=[input1,input2],outputs=distance)

   

#optimizer = tfa.optimizers.RectifiedAdam()
siamese_net.compile(loss=loss(margin=1), optimizer='rmsprop', metrics=['mse', tf.keras.metrics.RootMeanSquaredError()])

siamese_name='siamese'
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_mse',
                                                 factor = 0.2,
                                                 patience = 3,
                                                 verbose = 1,
                                                 min_delta = 1e-4,
                                                 min_lr = 1e-9,
                                                 mode = 'min')

earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_mse',
                                                 min_delta = 1e-4,
                                                 patience = 7,
                                                 mode = 'min',
                                                 restore_best_weights = True,
                                                 verbose = 1)

checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = siamese_name+'.h5',
                                                  monitor = 'val_mse', 
                                                  verbose = 1, 
                                                  save_best_only = True,
                                                  save_weights_only = True,
                                                  mode = 'min')

checkpointer2 = tf.keras.callbacks.ModelCheckpoint(filepath = siamese_name,
                                                  monitor = 'val_mse', 
                                                  verbose = 1, 
                                                  save_best_only = True,
                                                  save_weights_only = False,
                                                  mode = 'min')


callbacks = [earlystopping, reduce_lr, checkpointer, checkpointer2]


history = siamese_net.fit(train_gen, steps_per_epoch=len(train_gen),
                               epochs=100, validation_data=val_gen,
                               validation_steps=len(val_gen), shuffle=True,
                               verbose=True, callbacks = callbacks)

siamese_net.load_weights(siamese_name+'.h5')

for l in embbeding.layers:
    l.treinable=False 


##second part of training

def pre_process2(X):
    return np.expand_dims(X, axis=-1)/(max+0.0)

df_train = df[df['set'] == 'train']
df_val = df[df['set'] == 'validation']
df_test = df[df['set'] == 'test']

train_ids = df_train.path.values
train_classes = {}

for i, row in df_train.iterrows():
    train_classes[row['path']] = int(row['class'])
    
test_ids = df_test.path.values
test_classes = {}

for i, row in df_test.iterrows():
    test_classes[row['path']] = int(row['class'])

val_ids = df_val.path.values
val_classes = {}

for i, row in df_val.iterrows():
    val_classes[row['path']] = int(row['class'])


train_params = {'ids':train_ids, 'labels':train_classes, 'dim':dim, 'n_classes':nclasses, 'batch_size':batch_size, 'preprocessing':pre_process2}
train_gen = utils.TxtDataGenerator(**train_params)

test_params = {'ids':test_ids, 'labels':test_classes,
               'dim':dim, 'n_classes':nclasses, 'shuffle':False, 'batch_size':1, 'preprocessing':pre_process2}
test_gen = utils.TxtDataGenerator(**test_params)

val_params = {'ids':val_ids, 'labels':val_classes, 'dim':dim, 'n_classes':nclasses, 'batch_size':batch_size, 'preprocessing':pre_process2}
val_gen = utils.TxtDataGenerator(**val_params)    

_input = tf.keras.layers.Input(shape=(dim[0],dim[1],dim[2],dim[3],))
encod1 = embbeding(_input)
x = tf.keras.layers.Dense(40, activation='relu')(encod1)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(nclasses, activation='softmax')(x)
nmodel = tf.keras.models.Model(inputs=_input, outputs=x)

nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', utils.f1_m, utils.precision_m, utils.recall_m])
    
#callbacks
#set early sto
estop = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=20, mode='max', restore_best_weights=True)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_f1_m',
                                                 factor = 0.2,
                                                 patience = 3,
                                                 verbose = 1,
                                                 min_lr = 1e-8,
                                                 mode = 'max')

model_checkpoint_weights_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=path_weight,
    save_weights_only=True,
    monitor='val_f1_m',
    mode='max',
    save_best_only=True)

model_checkpoint_model_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=path_model,
    save_weights_only=False,
    monitor='val_f1_m',
    mode='max',
    save_best_only=True)

callbacks_list = [model_checkpoint_weights_callback,
                  model_checkpoint_model_callback,
                  estop,
                  reduce_lr]

history = nmodel.fit(train_gen, steps_per_epoch=len(train_gen),
                               epochs=50, validation_data=val_gen,
                               validation_steps=len(val_gen), shuffle=True,
                               verbose=True, callbacks=callbacks_list)
   

nmodel.load_weights(path_weight)

##FINE TUNING
for l in embbeding.layers:
    l.treinable=True
    
nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy', utils.f1_m, utils.precision_m, utils.recall_m])
    
#callbacks
#set early sto
estop = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=20, mode='max', restore_best_weights=True)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_f1_m',
                                                 factor = 0.2,
                                                 patience = 3,
                                                 verbose = 1,
                                                 min_lr = 1e-8,
                                                 mode = 'max')

model_checkpoint_weights_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=path_weight,
    save_weights_only=True,
    monitor='val_f1_m',
    mode='max',
    save_best_only=True)

model_checkpoint_model_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=path_model,
    save_weights_only=False,
    monitor='val_f1_m',
    mode='max',
    save_best_only=True)

callbacks_list = [model_checkpoint_weights_callback,
                  model_checkpoint_model_callback,
                  estop,
                  reduce_lr]

history = nmodel.fit(train_gen, steps_per_epoch=len(train_gen),
                               epochs=50, validation_data=val_gen,
                               validation_steps=len(val_gen), shuffle=True,
                               verbose=True, callbacks=callbacks_list)
    

#TESTING
#load the best weights
nmodel.load_weights(path_weight)
#test the model
report = utils.confusion_matrix(test_gen, nmodel)
evaluate = nmodel.evaluate(test_gen, return_dict=True)



if mlflow_activate:
    mlflow.set_experiment(experiment_name)
    mlflow.log_param("batch_size", batch_size)
    mlflow.log_param("dim", dim)
    mlflow.log_param("ds", path.split(os.path.sep)[-1])
    mlflow.log_param("optimizer", str(nmodel.optimizer).split()[0].split(".")[-1])
    mlflow.log_param("lr", nmodel.optimizer.lr.numpy())
    mlflow.log_artifact(os.getcwd()+os.path.sep+F"models\\{experiment_name}\\{model_save_name}")
    mlflow.log_text(report, os.getcwd()+os.path.sep+F"models\\{experiment_name}\\{model_save_name}_cm.txt")
    mlflow.log_metrics(evaluate)
    mlflow.log_figure(plt.gcf(), 'cm.png')
    mlflow.log_param("loss", str(nmodel.loss))
    mlflow.log_param("comments", comments)
    mlflow.log_param("dataset", path.split(os.path.sep)[-1])
    mlflow.end_run()
    