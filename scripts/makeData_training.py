# -*- coding: utf-8 -*-
"""[Prague] Exploring Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11O8rorVks833vt00VtbYTg1KF54EDEEi
"""

import pandas as pd


df = pd.read_csv(r'C:\Users\Gabriel\OneDrive - Universidade de Tras-os-Montes e Alto Douro\UTAD\Prague\csv_data\JuneLastWeek\TrainingPolygonsTable.txt')

#data exploration
import numpy as np

print('dtypes: ', df.dtypes)

columns = df.columns

for c in columns:
  print('min_{0}, {1}, max_{0}, {2}'.format(c,np.min(df[c].values), np.max(df[c].values)))

interval_class_points = 35
df_filtred = df[df['Xrel'] >= -1*interval_class_points]
df_filtred = df_filtred[df_filtred['Xrel'] <= interval_class_points]
df_filtred = df_filtred[df_filtred['Yrel'] >= -1*interval_class_points]
df_filtred = df_filtred[df_filtred['Yrel'] <= interval_class_points]

for c in columns:
  print('min_{0}, {1}, max_{0}, {2}'.format(c,np.min(df_filtred[c].values), np.max(df_filtred[c].values)))





unusual = {'511':[(-15, 5), (-5, 15)], '503':[(-15, 5), (-15, 5)], '505':[(-15, 5), (-5, 15)], '506':[(-25, 5), (-15, 5)], \
'531':[(-25, 5), (-5, 35)], '481':[(-15, 15), (-15, 15)], '482':[(-35, 15), (-15, 25)], '483	':[(-15, 25), (-25, 5)]}

print(df_filtred.columns)
print(len(df_filtred))

polygons = {}
def add_to_dict(tup, dict_polygons):
  if not int(tup[0]) in dict_polygons:
    dict_polygons[int(tup[0])] = []
  dict_polygons[int(tup[0])].append(tup[1])

[add_to_dict(x, polygons) for x in list(df_filtred.groupby(['LULUCF','Polygon_ID']).groups.keys())]

print(polygons)

polygons_size = {}

for x in list(polygons.keys()):
  polygons_size[x] = len(polygons[x])

print(polygons_size)

import os
import random
import copy
bands = ['nd_variance','nd', 'B12', 'B8A', 'B7', 'B11', 'B2', 'B6', 'B5', 'B3', 'B4', 'B8','elevation']
#factor of the data sampling
factor = 10 
size_window = 5 * (factor)
size = int(size_window/2)

#split dataset
train_percent = 0.7
val_percent = 0.15
test_percent = 0.15

train_set = {}
val_set = {}
test_set = {}

polygons_copy = copy.deepcopy(polygons)
print(polygons_size)

for x in list(polygons.keys()):
  #take the percentage
  train_quantity = round(train_percent*polygons_size[x])
  test_quantity = round(test_percent*polygons_size[x])
  val_quantity = round(val_percent*polygons_size[x])

  train_set[x] = []
  test_set[x] = []

  #create the train set
  for i in range(train_quantity):
    indice = round(random.random()*(len(polygons_copy[x])-1))
    train_set[x].append(polygons_copy[x][indice])
    del polygons_copy[x][indice]
  
  #create the test set
  for i in range(test_quantity):
    indice = round(random.random()*(len(polygons_copy[x])-1))
    test_set[x].append(polygons_copy[x][indice])
    del polygons_copy[x][indice]
  
  #copy the remainder elements to the val set
  val_set[x] = polygons_copy[x]

#prity quantities
[print(x, ': ', len(train_set[x])) for x in train_set.keys()]
print('-------')
[print(x, ': ', len(test_set[x])) for x in train_set.keys()]
print('-------')
[print(x, ': ', len(val_set[x])) for x in train_set.keys()]

mapping = {}

for x in list(train_set.values()):
  for y in x:
    mapping[y] = 'train'

for x in list(test_set.values()):
  for y in x:
    mapping[y] = 'test'

for x in list(val_set.values()):
  for y in x:
    mapping[y] = 'validation' 


print('quantity: ', len(df_filtred))
direcotory_to_save = r'C:\Users\Gabriel\OneDrive - Universidade de Tras-os-Montes e Alto Douro\UTAD\Prague\data_5_v4_noLines'

df_filtred['dataset'] = df_filtred['Polygon_ID'].map(mapping)

print(df_filtred['dataset'])
for i, row in df_filtred.iterrows():
  image = np.zeros((int(size_window/factor), int(size_window/factor), 13))

  #take the coordinates of the point and the class
  x = int(row['Xrel'])
  y = int(row['Yrel'])
  name = '('+str(x)+','+str(y)+')'
  #take the class
  class_landcover = int(row['LULUCF'])
  polygon_nb = int(row['Polygon_ID'])
  subset = 'train'
  
  #take points of polygon
  df_aux = df[df['Polygon_ID'] == row['Polygon_ID']]

  #crop the interested lines
  
  df_aux = df_aux[df_aux['Xrel'] >= x-size]
  df_aux = df_aux[df_aux['Xrel'] <= x+size]
  df_aux = df_aux[df_aux['Yrel'] >= y-size]
  df_aux = df_aux[df_aux['Yrel'] <= y+size]


  #take the min of each coordinate
  min_x = np.min(df_aux['Xrel'])
  min_y = np.min(df_aux['Yrel'])

  for k, row2 in df_aux.iterrows():
    xrel = int(((row2['Xrel'])-min_x)/factor)
    yrel = int(((row2['Yrel'])-min_y)/factor)
    image[xrel, yrel] = row2.values[5:]
  
  os.makedirs(os.path.join(direcotory_to_save, subset, str(class_landcover)), exist_ok=True)
  
  save = True
  if  str(polygon_nb) in unusual:
      if not (x>=unusual[str(polygon_nb)][0][0] and x <= unusual[str(polygon_nb)][0][1] and y>=unusual[str(polygon_nb)][1][0] and y <= unusual[str(polygon_nb)][1][1]):
          save = False
  if save:
      with open(os.path.join(direcotory_to_save, subset, str(class_landcover), str(polygon_nb)+'_'+str(int(i))+'_'+name+'.txt'), 'wb') as f: 
          np.save(f, image)
          f.close()